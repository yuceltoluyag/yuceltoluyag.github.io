Title: Wget ile Bir Web Sitesini DoÄŸru YÃ¶ntemle Ä°ndirme
Date: 2025-11-14 11:59
Category: Linux
Tags: wget, komut satÄ±rÄ±, linux, sorun giderme
Slug: wget-ile-web-sitesi-indirme
Authors: yuceltoluyag
Summary: Wget ile site indirmek bir sanattÄ±r. YanlÄ±ÅŸ komutla site Ã§Ã¶plÃ¼ÄŸe dÃ¶ner. KÄ±rÄ±k linkler olmadan, offline gezmek iÃ§in doÄŸru parametreleri gÃ¶steriyorum.
Image: images/wget-ile-web-sitesi-indirme-xl.webp
Lang: tr
Translation: false
Status: published

Linux'ta bir siteyi indirmek... aklÄ±ma direkt `wget` geliyor. Herkes tavsiye eder, biliyorsun. Ama o komut, yanlÄ±ÅŸ parametrelerle kullanÄ±lÄ±nca indirdiÄŸin ÅŸey resmen bir Ã§Ã¶plÃ¼ÄŸe dÃ¶ner. KÄ±rÄ±k linkler, boÅŸ resimler... geÃ§enlerde baÅŸÄ±ma geldi, sinir oldum. ğŸ˜¤ Ä°ÅŸte o yÃ¼zden buradayÄ±m, o sihirli kombinasyonu anlatmak iÃ§in.

AmacÄ±mÄ±z basit: Ä°ndirdiÄŸimiz siteyi bilgisayarda da sorunsuz gezebilelim, sunucuyu yormayalÄ±m ve dosyalarÄ± Ã¼st Ã¼ste yazdÄ±rmayalÄ±m.

Genelde iki yol var. Biri "recursive traversal", yani siteyi dallanÄ±p budaklanarak indirmek. DiÄŸeri ise sitenin `sitemap.xml` dosyasÄ±ndaki adresleri tek tek Ã§ekmek. Ä°kisi de iÅŸe yarar, ama farklÄ± durumlar iÃ§in.

## O MeÅŸhur "Dal Budak" Ä°ndirme Ä°ÅŸlemi

Tabii ki `wget` kullanacaÄŸÄ±z. GNU Wget, Web'den dosyalarÄ± etkileÅŸimsiz olarak indirmek iÃ§in Ã¼cretsiz bir yardÄ±mcÄ± programdÄ±r...

...BÃ¶yle demek Ã§ok resmi geliyor bana. Dur, ÅŸÃ¶yle dÃ¼zelteyim: `wget`, Linux'un en eski, en saÄŸlam askerlerinden biridir. Ã‡oÄŸu daÄŸÄ±tÄ±mda hazÄ±r zaten.

Peki, bu askeri doÄŸru kullanalÄ±m?

### Sihirli SÃ¶zcÃ¼kler ve AnlamlarÄ±

Teknik makaleler gibi her parametreyi tek tek anlatmak istemiyorum. AÅŸÄ±rÄ± sÄ±kÄ±cÄ± olur. Bunun yerine, bu komutun hangi parÃ§asÄ±nÄ±n ne iÅŸe yaradÄ±ÄŸÄ±nÄ±, hani bir arabanÄ±n parÃ§alarÄ± gibi dÃ¼ÅŸÃ¼nelim.

**Ã–nce "Kibar Olma" KÄ±smÄ±:**
Ben genellikle `--wait=2` ile baÅŸlarÄ±m. Neden mi? Bir keresinde hÄ±zlÄ± indirmiÅŸtim de site sahibi beni banlamÄ±ÅŸtÄ±. O gÃ¼nden sonra "ne kadar nazik olursan o kadar iyi" prensibiyle hareket ediyorum. Bu komut, wget'e her dosya indirmeden Ã¶nce 2 saniye mola vermesini sÃ¶ylÃ¼yor. Hatta bazen 3 saniyeye Ã§Ä±karÄ±yorum, kendime gelme fÄ±rsatÄ± buluyorum o sÄ±rada.

AklÄ±ma geldi, acaba o site sahibi hala beni engelliyor mudur? Neyse, konuya dÃ¶neyim.

Buna benzer bir ÅŸekilde `--limit-rate=20K` de var. Bu, indirme hÄ±zÄ±nÄ± saniyede 20 kilobaytle sÄ±nÄ±rlÄ±yor. Yani wget'e "hÄ±rs yapma, yavaÅŸ ve istikrarlÄ± ol" demiÅŸ oluyorsun. Yoksa hem sunucunun bandÄ±nÄ± tÃ¼ketirsin hem de kendi internetin kitlenir. Bu komut, Ã¶zellikle bÃ¼yÃ¼k siteler indirirken hayat kurtarÄ±cÄ± oluyor.

**Åimdi "Ä°ÅŸi DoÄŸru Yapma" KÄ±smÄ±:**
`--recursive` komutu, iÅŸin sihrinin baÅŸladÄ±ÄŸÄ± yer. Bu, sitenin bir dalÄ±ndan diÄŸerine atlayarak tÃ¼m sayfalarÄ± indirmesini saÄŸlÄ±yor. TavÅŸan deliÄŸine inmek gibi bir ÅŸey. ğŸ‡ VarsayÄ±lan olarak 5 derine iniyor, eÄŸer site daha derinse `--level=inf` diyerek sonsuza kadar indirebilirsin (ama dikkatli ol, Ã§ok uzun sÃ¼rebilir\!). Emin deÄŸilim ama sanÄ±rÄ±m `--level=0` da sonsuz iÃ§in geÃ§erli olabilir, kontrol etmekte fayda var.

Peki ya sayfanÄ±n resimleri, CSS'leri? Ä°ÅŸte `--page-requisites` burada devreye giriyor. Bu komut, sayfanÄ±n "gerekli olan her ÅŸeyini" - resim, css, javascript gibi - beraberinde getirmesini saÄŸlÄ±yor. Bunu yapmazsan, indirdiÄŸin site sanki eÅŸyasÄ±z, boÅŸ bir ev gibi olur, hiÃ§bir ÅŸey dÃ¼zgÃ¼n gÃ¶rÃ¼nmez. ğŸ‘»

**Son DokunuÅŸlar:**
Ä°ndirme bittikten sonra linkler bozulursa? Ä°ÅŸte `--convert-links` tam da bunun iÃ§in. Bu komut, indirdiÄŸin sitenin kendi bilgisayarÄ±nda Ã§alÄ±ÅŸmasÄ±nÄ± saÄŸlÄ±yor. Linkleri yerel dosyalara yÃ¶nlendiriyor. Yoksa tÄ±kladÄ±ÄŸÄ±n link boÅŸuna aÃ§Ä±lÄ±r.

`--adjust-extension` de Ã§ok faydalÄ±. EÄŸer indirilen sayfanÄ±n uzantÄ±sÄ± `.html` deÄŸilse, wget onun sonuna `.html` ekliyor. Bu, yerel gezinme iÃ§in Ã§ok Ã¶nemli.

`--no-clobber` ise kopyalarÄ± engelliyor. Yani bir dosyayÄ± indirdin, sonra tekrar Ã§alÄ±ÅŸtÄ±rÄ±rsÄ±n, wget eskinin Ã¼zerine yazmaz, "bu var" deyip geÃ§er. Bazen iÅŸe yarar.

!!! note "Ä°pucu âš¡ Hemen ÅŸurada kÃ¼Ã§Ã¼k bir not bÄ±rakayÄ±m: `-e robots=off` parametresi, sitenin `robots.txt` dosyasÄ±nÄ± umursamasÄ±nÄ± engeller. Yani "ben bir botum ama kurallarÄ±na uymayacaÄŸÄ±m" demiÅŸ olursun. Bunu dikkatli kullan, bazÄ± siteler bunu sevmeyebilir."

### Peki, Bu Komutun Son Hali NasÄ±l OlmalÄ±?

Ä°ÅŸte tÃ¼m bu tecrÃ¼belerimden sonra bir araya getirdiÄŸim o sihirli komut. Buna "sÃ¼per indirme bÃ¼yÃ¼sÃ¼" diyebiliriz:

```bash
wget --wait=2 \
     --level=inf \
     --limit-rate=20K \
     --recursive \
     --page-requisites \
     --user-agent=Mozilla \
     --no-parent \
     --convert-links \
     --adjust-extension \
     --no-clobber \
     -e robots=off \
     https://example.com
```

### Ã–rnek Deneme

Hadi `https://example.com` sitesini indirelim ve `wget`'in ne kadar "gÃ¼rÃ¼ltÃ¼cÃ¼" olduÄŸunu gÃ¶relim.

```bash
$ wget --wait=2 --limit-rate=20K --recursive --page-requisites --user-agent=Mozilla --no-parent --convert-links --adjust-extension --no-clobber https://example.com
--2017-06-30 19:48:46--  https://example.com/
example.com (example.com) Ã§Ã¶zÃ¼mleniyor... 93.184.216.34
example.com (example.com)[93.184.216.34]:443 baÄŸlanÄ±lÄ±yor... baÄŸlandÄ±.
HTTP isteÄŸi gÃ¶nderildi, yanÄ±t bekleniyor... 200 OK
Uzunluk: 1270 (1,2K) [text/html]
â€˜example.com/index.htmlâ€™ olarak kaydediliyor.

example.com/index.html            100%[===========================================================>]   1,24K  --.-KB/s    in 0,003s

2017-06-30 19:48:46 (371 KB/s) - â€˜example.com/index.htmlâ€™ kaydedildi [1270/1270]

BÄ°TTÄ° --2017-06-30 19:48:46--
Toplam harcanan sÃ¼re: 0,6s
Ä°ndirilen: 1 dosya, 1,2K in 0,003s (371 KB/s)
example.com/index.html iÃ§indeki linkler dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor... yapÄ±lacak bir ÅŸey yok.
1 dosyadaki linkler 0 saniye iÃ§inde dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼.
$ tree example.com/
example.com/
â””â”€â”€ index.html

0 directories, 1 file
```

BakÄ±n ne kadar detaylÄ± rapor veriyor, her adÄ±mÄ± sÃ¶ylÃ¼yor. Bu, hata ayÄ±klamak iÃ§in Ã§ok gÃ¼zel.

!!! tip "KÄ±sa Yol: Wget Mirror Åimdi diyeceksiniz ki "Ay bunlarÄ± hepsini yazmak Ã§ok uzun, kÄ±sa yolu yok mu?". Var tabii `wget` zaten kullanÄ±ÅŸlÄ± bir `--mirror` parametresiyle gelir. Bu parametre, `-r -l inf -N` kullanmakla aynÄ± anlama gelir. Yani recursive, sonsuz derinlikte ve zaman damgalÄ± indirme yapar."

## 2. YÃ¶ntem: Daha Zeki Olmak (Sitemap KullanÄ±mÄ±)

BaÅŸka bir yaklaÅŸÄ±m ise sitenin recursive traversal yapmaktan kaÃ§Ä±nmak ve sitenin `sitemap.xml` dosyasÄ±ndaki tÃ¼m URL'leri indirmektir. Bu, Ã¶zellikle sadece belirli sayfalarÄ± istediÄŸinizde Ã§ok daha temiz bir yÃ¶ntemdir.

### Sitemap'tan URL'leri Filtreleme

Bir sitemap dosyasÄ± genellikle ÅŸu ÅŸekildedir:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.sitemaps.org/schemas/sitemap/0.9 http://www.sitemaps.org/schemas/sitemap/0.9/sitemap.xsd" xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
<url>
<loc>https://yuceltoluyag.github.io/</loc>
<lastmod>2014-09-15T00:00:00-03:00</lastmod>
</url>
<url>
<loc>https://yuceltoluyag.github.io/en</loc>
<lastmod>2014-09-15T00:00:00-03:00</lastmod>
</url>
</urlset>
```

Burada `sitemap.xml` iÃ§indeki tÃ¼m URL'leri almalÄ±yÄ±z. Bunun iÃ§in `grep` kullanabiliriz: `grep "<loc>" sitemap.xml`.

### Loc Etiketlerini KaldÄ±rma

Åimdi gereksiz etiketleri temizleyelim: `sed -e 's/<[^>]*>//g'`

### Hep Bir Arada

Ã–nceki iki komuttan sonra elimizde bir URL listesi oldu ve bu liste, `wget -i` komutunun okuduÄŸu parametredir.

```bash
wget -i $(grep "<loc>" sitemap.xml | sed -e 's/<[^>]*>//g')
```

Ve `wget` bunlarÄ± sÄ±rayla indirmeye baÅŸlayacaktÄ±r. Bu yÃ¶ntem, sadece ihtiyacÄ±mÄ±z olan sayfalarÄ± indirmek iÃ§in Ã§ok daha verimli olabilir.[^1]

## Ä°ÅŸin Ã–zÃ¼...

Yani, iÅŸin Ã¶zÃ¼, wget gerÃ§ekten bir hazine. BaÅŸka bir GUI aracÄ± kullanmanÄ±za gerek kalmayacak, komut satÄ±rÄ±nda aklÄ±nÄ±za gelebilecek her ÅŸeyi yapabilirsiniz. Sadece doÄŸru parametreler iÃ§in manual'Ä±na bir gÃ¶z atmanÄ±z yeterli.

YukarÄ±daki parametre kombinasyonu sayesinde, elinizde tamamen gezinebilir, yerel bir kopya olur.

!!! warning "Dikkat âš ï¸ Bir uyarÄ± yapayÄ±m, `.html` uzantÄ±larÄ± her iÅŸe yaramayabilir. Bazen wget'in Content Type'a gÃ¶re uzantÄ± Ã¼retmesini isteyebilirsiniz, bazen de "pretty URL"ler (gÃ¼zel URL'ler) kullandÄ±ÄŸÄ±nÄ±zda wget'in uzantÄ± Ã¼retmesini Ã¶nlemeniz gerekebilir. Bu duruma gÃ¶re ayarlamanÄ±z gerekir."

UmarÄ±m bu yazÄ± iÅŸinize yarar, bol indirmeler

[^1]: AslÄ±nda bu yÃ¶ntemi ilk keÅŸfettiÄŸimde Ã§ok sevinmiÅŸtim. Ã‡Ã¼nkÃ¼ Ã§ok bÃ¼yÃ¼k bir sitede sadece belirli bir kategorideki sayfalarÄ± indirmem gerekiyordu ve bu beni saatlerce sÃ¼rebilecek bir iÅŸten kurtardÄ±.
